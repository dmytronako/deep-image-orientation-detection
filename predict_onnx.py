import torch
import os
import argparse
import logging
import time
import onnxruntime
import numpy as np

import config
import torchvision.transforms as T
from src.utils import setup_logging, load_image_safely

def predict_single_image_onnx(ort_session, image_path, image_transforms):
    """Predicts orientation for a single image file using the ONNX model and logs the time taken."""
    
    start_time = time.time() # Start timer

    try:
        image = load_image_safely(image_path)
    except FileNotFoundError:
        print(f"File not found: {image_path}")
        return
    except Exception as e:
        print(f"Error opening image {image_path}: {e}")
        return

    # Apply transformations and convert to NumPy array for ONNX Runtime
    input_tensor = image_transforms(image).unsqueeze(0).cpu().numpy()
    
    # Get input name from the ONNX session
    ort_inputs = {ort_session.get_inputs()[0].name: input_tensor}
    
    # Run inference
    ort_outs = ort_session.run(None, ort_inputs)
    
    # Process output: ONNX Runtime returns a list of outputs, we need the first one
    output = torch.from_numpy(ort_outs[0])
    _, predicted_idx = torch.max(output, 1)
        
    predicted_class = predicted_idx.item()
    result = config.CLASS_MAP[predicted_class]
    
    end_time = time.time() # End timer
    duration = end_time - start_time
    
    print(f"-> Image: '{os.path.basename(image_path)}' | Prediction: {result} (Took {duration:.4f} seconds)")


def run_prediction_onnx(args):
    """Main ONNX prediction routine."""
    setup_logging()
    
    if not os.path.exists(args.model_path):
        logging.error(f"ONNX model file not found at {args.model_path}.")
        return

    # Define the same transformations used during validation.
    image_transforms = T.Compose([
        T.Resize((config.IMAGE_SIZE + 32, config.IMAGE_SIZE + 32)),
        T.CenterCrop(config.IMAGE_SIZE),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Define a priority list for execution providers.
    # ONNX Runtime will try to use the first one in this list that is available on the system.
    PREFERRED_PROVIDERS = [
        #'TensorrtExecutionProvider',  # For NVIDIA GPUs with TensorRT (commenting out for now)
        'CUDAExecutionProvider',      # For NVIDIA GPUs
        'MpsExecutionProvider',       # For Apple Silicon (M1/M2/M3) GPUs
        'ROCmExecutionProvider',      # For AMD GPUs
        'CoreMLExecutionProvider',    # For Apple devices (can use Neural Engine)
        'CPUExecutionProvider'        # Universal fallback
    ]

    try:
        available_providers = onnxruntime.get_available_providers()
        logging.info(f"Available ONNX Runtime providers: {available_providers}")

        chosen_provider = None
        for provider in PREFERRED_PROVIDERS:
            if provider in available_providers:
                chosen_provider = provider
                break
        
        if not chosen_provider:
            logging.warning(
                "No preferred provider found. Defaulting to CPUExecutionProvider. "
                "This should not happen as CPUExecutionProvider is always available."
            )
            chosen_provider = 'CPUExecutionProvider'

        logging.info(f"Attempting to load ONNX model with provider: {chosen_provider}")
        
        # Load the ONNX model with the single, highest-priority available provider
        ort_session = onnxruntime.InferenceSession(args.model_path, providers=[chosen_provider])
        
        actual_provider = ort_session.get_providers()[0]
        logging.info(f"Successfully loaded ONNX model from {args.model_path} using provider: {actual_provider}")
        
        if chosen_provider != actual_provider and actual_provider == 'CPUExecutionProvider':
             logging.warning(f"Warning: ONNX Runtime fell back to CPU. The chosen provider '{chosen_provider}' might not be correctly configured.")

    except Exception as e:
        logging.error(f"Error loading ONNX model {args.model_path}: {e}")
        logging.error(
            "If you are trying to use a GPU provider (CUDA, TensorRT, ROCm, MPS), "
            "please ensure the correct onnxruntime package is installed and drivers are up to date."
        )
        return

    input_path = args.input_path
    if not os.path.exists(input_path):
        logging.error(f"Input path does not exist: {input_path}")
        return

    if os.path.isfile(input_path):
        print(f"Processing single image: {input_path}")
        predict_single_image_onnx(ort_session, input_path, image_transforms)
    elif os.path.isdir(input_path):
        print(f"Processing all images in directory: {input_path}")
        total_dir_start_time = time.time() # Start timer for the entire directory
        image_files = [f for f in os.listdir(input_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        if not image_files:
            print(f"No image files found in directory: {input_path}")
            return

        for image_file in image_files:
            full_path = os.path.join(input_path, image_file)
            predict_single_image_onnx(ort_session, full_path, image_transforms)
        
        total_dir_end_time = time.time() # End timer
        total_duration = total_dir_end_time - total_dir_start_time
        print(f"Finished processing directory '{input_path}'. Total time: {total_duration:.4f} seconds for {len(image_files)} images.")
    else:
        print(f"Input path is not a valid file or directory: {input_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Predict image orientation using an ONNX model.")
    parser.add_argument('--input_path', type=str, required=True, help='Path to an image file or a directory of images.')
    parser.add_argument('--model_path', type=str, default=os.path.join(config.MODEL_SAVE_DIR, f"{config.MODEL_NAME}.onnx"), help='Path to the ONNX model file.')
    
    args = parser.parse_args()
    run_prediction_onnx(args)
